{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring Spark...\nLoading file  ./movies.csv ...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o90.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/movies.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:60)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-843fa5138243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Load the movies and ratings database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./movies.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoveHeader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./ratings_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoveHeader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-843fa5138243>\u001b[0m in \u001b[0;36mreadCSV\u001b[0;34m(fname, removeHeader, separator)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremoveHeader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mfirstline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfirstline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseparator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \"\"\"\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \"\"\"\n\u001b[1;32m   1279\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0mtotalParts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \"\"\"\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o90.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/movies.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:60)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define some constants\n",
    "RATING_MIN = 0.5\n",
    "RATING_MAX = 5.0\n",
    "RATING_RANGE = RATING_MAX - RATING_MIN\n",
    "\n",
    "# Enable crossjoins\n",
    "print(\"Configuring Spark...\")\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", True)\n",
    "\n",
    "# Define some helper functions\n",
    "def readCSV(fname, removeHeader=False, separator=','):\n",
    "    print(\"Loading file \", fname, \"...\")\n",
    "    rdd = sc.textFile(fname)\n",
    "    if removeHeader:\n",
    "        firstline = rdd.first()\n",
    "        rdd = rdd.filter(lambda x: x != firstline)\n",
    "    return rdd.map(lambda x: x.split(separator))\n",
    "\n",
    "# Load the movies and ratings database\n",
    "movies = readCSV(\"./movies.csv\", removeHeader=True)\n",
    "ratings = readCSV(\"./ratings_train.csv\", removeHeader=True)\n",
    "\n",
    "# Print the first entries to debug whether the data is loaded correctly\n",
    "print(\"First movie:\", movies.first())\n",
    "print(\"First rating:\", ratings.first())\n",
    "\n",
    "# Parse the movie genres\n",
    "# [id, name, genres[]]\n",
    "movies = movies.map(lambda x: [x[0], x[1], x[2].split('|')])\n",
    "print(\"First movie, processed:\", movies.first())\n",
    "\n",
    "# Parse the rating data\n",
    "# [user_id, movie_id, rating, timestamp]\n",
    "ratings = ratings.map(lambda x: x[0].split('::'))\n",
    "print(\"First rating, processed:\", ratings.first())\n",
    "\n",
    "print(\"Caching ratings...\")\n",
    "ratings = ratings.cache()\n",
    "\n",
    "# Select the user to suggest movies for\n",
    "client = ratings.first();\n",
    "client_id = client[0];\n",
    "#print(\"Determining movie suggestions for user\", client[0], \"...\")\n",
    "#\n",
    "#def addToSet(input_set, value):\n",
    "#    input_set.add(value)\n",
    "#    return input_set\n",
    "#\n",
    "## Group all ratings by their user keys\n",
    "#user_ratings = ratings.map(lambda x: (x[0], tuple(x[1:])))\\\n",
    "#                   .aggregateByKey(\\\n",
    "#                       set(), # initial value for an accumulator \\\n",
    "#                       addToSet, # function to add a value to an accumulator \\\n",
    "#                       lambda r1, r2: r1.union(r2) # function to merge two accumulators \\\n",
    "#                   )\n",
    "#\n",
    "## Get the ratings for the selected client\n",
    "#client_ratings = user_ratings.lookup(client_id)\n",
    "#print(\"Client ratings:\", client_ratings)\n",
    "#\n",
    "## The selected client must not be in the list of user ratings\n",
    "#user_ratings = user_ratings.filter(lambda x: x[0] != client_id)\n",
    "#\n",
    "#print(\"Caching user ratings...\")\n",
    "#user_ratings = user_ratings.cache()\n",
    "\n",
    "# Create a data frame with all the ratings\n",
    "ratings_df = spark.createDataFrame(ratings, ['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "client_ratings_df = ratings_df.filter(ratings_df.user_id == client_id).alias(\"client\")\n",
    "user_ratings_df = ratings_df.filter(ratings_df.user_id != client_id).alias(\"other\")\n",
    "movies_df = spark.createDataFrame(movies, ['id', 'name', 'genres'])\\\n",
    "    .select(\"id\", \"name\").alias(\"movies\")\n",
    "\n",
    "#####################################\n",
    "print(\"user_ratings_df:\")\n",
    "user_ratings_df.show(5)\n",
    "\n",
    "# Join the movies watched by the client and the other user\n",
    "aaa = user_ratings_df.join(client_ratings_df, on = \"movie_id\", how = \"inner\")\n",
    "\n",
    "# Determine the rating distance for each user/movie pair, and normalize it\n",
    "aaa = aaa.select(\\\n",
    "                 col(\"client.user_id\").alias(\"sim_client_user_id\"),\\\n",
    "                 col(\"other.user_id\").alias(\"sim_other_user_id\"),\\\n",
    "                 \"movie_id\",\\\n",
    "                 (abs(col(\"client.rating\") - col(\"other.rating\")) - RATING_RANGE / 2.0).alias(\"rating_dist_norm\")\\\n",
    "                )\n",
    "\n",
    "#####################################\n",
    "print(\"aaa:\")\n",
    "aaa.show(5)\n",
    "\n",
    "# For each client, calculate the similarity to the other users\n",
    "users_similarity = aaa.groupBy(\"sim_client_user_id\", \"sim_other_user_id\")\\\n",
    "    .agg(sum(\"rating_dist_norm\").alias(\"similarity\")).alias(\"similarity\")\n",
    "\n",
    "## Create a list of users\n",
    "print(\"Creating list of users...\")\n",
    "user_list_df = ratings_df.select(col(\"user_id\").alias(\"list_user_id\")).distinct()\n",
    "# TODO: Do not limit to 1 in production!!!\n",
    "user_list_df = user_list_df.limit(1)\n",
    "\n",
    "#####################################\n",
    "print(\"user_list_df:\")\n",
    "user_list_df.show(1)\n",
    "\n",
    "suggestions = user_list_df.join(\\\n",
    "                                movies_df.select(\\\n",
    "                                                 col(\"id\").alias(\"sug_movie_id\")\\\n",
    "                                                ),\\\n",
    "                                on = col(\"sug_movie_id\") != True,\\\n",
    "                                how = \"inner\")\n",
    "\n",
    "#####################################\n",
    "print(\"suggestions:\")\n",
    "suggestions.show(5)\n",
    "\n",
    "suggestions = suggestions.join(user_ratings_df,\\\n",
    "                               on = [col(\"sug_movie_id\") == col(\"movie_id\"),\\\n",
    "                                     col(\"list_user_id\") != col(\"user_id\")],\\\n",
    "                               how = \"inner\")\n",
    "\n",
    "suggestions = suggestions.join(\\\n",
    "                               users_similarity,\\\n",
    "                               on = [col(\"list_user_id\") == col(\"sim_client_user_id\"),\\\n",
    "                                     col(\"user_id\") == col(\"sim_other_user_id\")],\\\n",
    "                              how = \"inner\")\n",
    "\n",
    "# TODO: Filter movies that have been watched already here\n",
    "\n",
    "\n",
    "\n",
    "#unwatched.filter(client_ratings_df\\\n",
    "#                 .select(client_ratings_df.movie_id)\\\n",
    "#                 .where(client_ratings_df.movie_id == unwatched.id)\\\n",
    "#                 .limit(1)\\\n",
    "#                 .count() == 0)\n",
    "\n",
    "\n",
    "suggestions.show(1000)\n",
    "\n",
    "\n",
    "#unwatched_df = client_ratings_df.join(movies_df, on = client_ratings_df.movie_id != movies_df.id, how = \"left\")\n",
    "#unwatched_df.show(10000)\n",
    "\n",
    "#def abc_test(user):\n",
    "#    print(\"For user\", user.user_id)\n",
    "#    #user_movies = movies_df.filter(\\\n",
    "#    #                               ratings_df.where(col(\"user_id\") == user.user_id)\\\n",
    "#    #                                   .filter(movies_df.id == ratings_df.movie_id)\\\n",
    "#    #                                   .limit(1)\\\n",
    "#    #                                   .count() == 0\n",
    "#    #                              )\n",
    "#    #\n",
    "#    #user_movies.show(1000)\n",
    "#    pass\n",
    "#\n",
    "#print(\"Number of users:\", user_list_df.count())\n",
    "#user_list_df.foreach(abc_test)\n",
    "\n",
    "#unwatched_df = client_ratings_df.join(movies_df, on = client_ratings_df.movie_id == movies_df.id, how = \"outer\")\n",
    "#unwatched_df = client_ratings_df.crossJoin(movies_df)\n",
    "    \n",
    "#unwatched_df.sort(asc(\"user_id\"), asc(\"movie_id\")).show(10000)\n",
    "#unwatched_df.show(10000)\n",
    "\n",
    "#aaa.sort(desc('rating_dist_norm')).show(100)\n",
    "#aaa.show(100)\n",
    "\n",
    "# Aggregate o\n",
    "# user_ratings.mapValues(lambda x: )\n",
    "\n",
    "# TEST: Calculate rating averages\n",
    "# print(\"Calculating ratings average...\")\n",
    "# user_ratings = ratings.map(lambda x: float(x[2]))\n",
    "# print(\"Avg rating:\", user_ratings.sum()/user_ratings.count())\n",
    "\n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all dataset files...\n",
      "Loading file  ./ratings_full.csv ...\n",
      "Loading file  ./ratings_small.csv ...\n",
      "Loading file  ./movies_full.csv ...\n",
      "Loading file  ./movies_small.csv ...\n",
      "Parsing datasets...\n",
      "There are 24404096 ratings in the complete dataset.\n",
      "There are 100004 ratings in the small dataset.\n",
      "There are 40110 movies in the complete dataset.\n",
      "There are 9125 movies in the small dataset.\n",
      "Calibrating by training on small dataset...\n",
      "Training small dataset with  4 , has RMSE: 0.9405925542574993\n",
      "Training small dataset with  8 , has RMSE: 0.9451745059144596\n",
      "Training small dataset with  12 , has RMSE: 0.9435903947376889\n",
      "Trained small dataset calibration, best rank: 4\n",
      "Training complete dataset...\n",
      "Testing trained complete dataset...\n",
      "Trained complete dataset test, RMSE: 0.8318525567660949\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "# Configuration.                       #\n",
    "########################################\n",
    "\n",
    "# Paths to the available data sets\n",
    "DATASET_RATINGS_PATH = \"./ratings_full.csv\"\n",
    "DATASET_RATINGS_SMALL_PATH = \"./ratings_small.csv\"\n",
    "DATASET_MOVIES_PATH = \"./movies_full.csv\"\n",
    "DATASET_MOVIES_SMALL_PATH = \"./movies_small.csv\"\n",
    "\n",
    "# The minimum number of reviews a movie must have, to recommend it reliably\n",
    "REVIEW_MIN_AMOUNT = 20\n",
    "\n",
    "# Split ratio for the small training/calibration data\n",
    "TRAIN_SMALL_SPLIT = [6, 2, 2]\n",
    "\n",
    "# Split ratio for the complete training data\n",
    "TRAIN_COMPLETE_SPLIT = [7, 3]\n",
    "\n",
    "# Seed to use for training\n",
    "TRAIN_SEED = 5\n",
    "\n",
    "# Ranks to use for training\n",
    "TRAIN_RANKS = [4, 8, 10, 12]\n",
    "\n",
    "# Number of iterations to train for\n",
    "TRAIN_ITERATIONS = 12\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function definitions.                #\n",
    "########################################\n",
    "\n",
    "# Define some helper functions\n",
    "def readCSV(fname, removeHeader=False, separator=','):\n",
    "    print(\"Loading file \", fname, \"...\")\n",
    "    rdd = sc.textFile(fname)\n",
    "    if removeHeader:\n",
    "        firstline = rdd.first()\n",
    "        rdd = rdd.filter(lambda x: x != firstline)\n",
    "    return rdd.map(lambda x: x.split(separator))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "# Load datasets.                       #\n",
    "########################################\n",
    "\n",
    "print(\"Loading all dataset files...\")\n",
    "\n",
    "# Load the ratings data\n",
    "ratings_data = readCSV(DATASET_RATINGS_PATH, removeHeader=True)\n",
    "small_ratings_data = readCSV(DATASET_RATINGS_SMALL_PATH, removeHeader=True)\n",
    "\n",
    "# Load the movies data\n",
    "movies_data = readCSV(DATASET_MOVIES_PATH, removeHeader=True)\n",
    "small_movies_data = readCSV(DATASET_MOVIES_SMALL_PATH, removeHeader=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "# Parse datasets.                      #\n",
    "########################################\n",
    "\n",
    "print(\"Parsing datasets...\")\n",
    "\n",
    "# Parse the complete ratings set\n",
    "# [user_id, movie_id, rating, timestamp] -> (user_id, movie_id, rating)\n",
    "ratings_data = ratings_data.map(lambda x: (int(x[0]), int(x[1]), float(x[2])))\\\n",
    "    .cache()\n",
    "print(\"There are\", ratings_data.count(), \"ratings in the complete dataset.\")\n",
    "\n",
    "# Parse the small ratings set\n",
    "small_ratings_data = small_ratings_data.map(lambda x: (int(x[0]), int(x[1]), float(x[2])))\\\n",
    "    .cache()\n",
    "print(\"There are\", small_ratings_data.count(), \"ratings in the small dataset.\")\n",
    "\n",
    "# Parse the complete movies set\n",
    "# [id, title, genres[]] -> (id, title)\n",
    "movies_data = movies_data.map(lambda x: (int(x[0]), x[1]))\\\n",
    "    .cache()\n",
    "print(\"There are\", movies_data.count(), \"movies in the complete dataset.\")\n",
    "      \n",
    "# Parse the small ratings set\n",
    "small_movies_data = small_movies_data.map(lambda x: (int(x[0]), x[1]))\\\n",
    "    .cache()\n",
    "print(\"There are\", small_movies_data.count(), \"movies in the small dataset.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "# Calibrate machine learning.          #\n",
    "########################################\n",
    "\n",
    "print(\"Calibrating by training on small dataset...\")\n",
    "\n",
    "# Some parameters\n",
    "regularization_parameter = 0.1\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "\n",
    "# Create some training sets based on the small data\n",
    "training_set, validation_set, test_set = small_ratings_data\\\n",
    "        .randomSplit(TRAIN_SMALL_SPLIT, seed = 0)\n",
    "prediction_validation = validation_set\\\n",
    "        .map(lambda x: tuple(x[0:2]))\n",
    "prediction_test = test_set\\\n",
    "        .map(lambda x: tuple(x[0:2]))\n",
    "\n",
    "# Rememer the minimum error value, best rank and iteration\n",
    "min_error = float(\"inf\")\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "\n",
    "# Loop through each rank to train\n",
    "for rank in TRAIN_RANKS:\n",
    "    # Train on the selected rank\n",
    "    model = ALS.train(training_set,\\\n",
    "                      rank,\\\n",
    "                      seed = TRAIN_SEED,\\\n",
    "                      iterations = TRAIN_ITERATIONS,\\\n",
    "                      lambda_ = regularization_parameter)\n",
    "    \n",
    "    # Make some predictions to test\n",
    "    predictions = model\\\n",
    "            .predictAll(prediction_validation)\\\n",
    "            .map(lambda x: (tuple(x[0:2]), x[2]))\n",
    "    rates_predictions = validation_set\\\n",
    "            .map(lambda x: ((int(x[0]), int(x[1])), float(x[2])))\\\n",
    "            .join(predictions)\n",
    "            \n",
    "    # Calculate the predeiction error value, and store it\n",
    "    error = math.sqrt(rates_predictions\\\n",
    "                      .map(lambda x: (x[1][0] - x[1][1]) ** 2)\\\n",
    "                      .mean())\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    \n",
    "    # Update the minimum error value and the best rank\n",
    "    print(\"Training small dataset with rank\", rank, \"which has RMSE:\", error)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print(\"Trained small dataset calibration, best rank:\", best_rank)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "# Train on complete dataset.           #\n",
    "########################################\n",
    "\n",
    "print(\"Training complete dataset...\")\n",
    "\n",
    "# Randomly split the data, to use for training and testing\n",
    "training_set, test_set = ratings_data.randomSplit(TRAIN_COMPLETE_SPLIT,\\\n",
    "                                                  seed = 0)\n",
    "\n",
    "# Train using the data sets\n",
    "complete_model = ALS.train(training_set,\\\n",
    "                           best_rank,\\\n",
    "                           seed = seed,\\\n",
    "                           iterations = iterations,\\\n",
    "                           lambda_ = regularization_parameter)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "# Test trained data.                   #\n",
    "########################################\n",
    "\n",
    "print(\"Testing trained complete dataset...\")\n",
    "\n",
    "# Create a test prediction set\n",
    "test_predict_set = test_set.map(lambda x: tuple(x[0:2]))\n",
    "\n",
    "# Make come predictions to test\n",
    "predictions = complete_model\\\n",
    "        .predictAll(test_predict_set)\\\n",
    "        .map(lambda x: (tuple(x[0:2]), x[2]))\n",
    "rates_predictions = test_set\\\n",
    "        .map(lambda x: ((int(x[0]), int(x[1])), float(x[2])))\\\n",
    "        .join(predictions)\n",
    "\n",
    "# Calculate the error value\n",
    "error = math.sqrt(\\\n",
    "                  rates_predictions\\\n",
    "                          .map(lambda x: (x[1][0] - x[1][1]) ** 2)\\\n",
    "                          .mean())\n",
    "    \n",
    "print(\"Trained complete dataset test, RMSE:\", error)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "# Calculate movie review count.        #\n",
    "########################################\n",
    "\n",
    "# Helper method to count the number of ratings for each movie\n",
    "def calc_avg_count(id_ratings):\n",
    "    number = len(id_ratings[1])\n",
    "    return id_ratings[0],\\\n",
    "        (number, float(sum(x for x in id_ratings[1])) / number)\n",
    "\n",
    "# Count the number of ratings per movie\n",
    "movie_id_ratings = (ratings_data\\\n",
    "                            .map(lambda x: (x[1], x[2]))\\\n",
    "                            .groupByKey())\n",
    "movie_id_ratings_avg = movie_id_ratings.map(calc_avg_count)\n",
    "movie_id_ratings_count = movie_id_ratings_avg.map(lambda x: (x[0], x[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID: 1\n",
      "Best recommended movies:\n",
      "('\"Lonely Wife', 5.24120978478798, 25)('Napoléon (1927)', 5.2280430548581815, 28)(\"Shall We Kiss? (Un baiser s'il vous plait) (2007)\", 5.126939654881557, 22)('Landscape in the Mist (Topio stin omichli) (1988)', 5.0258980522406915, 40)('Connections (1978)', 5.022598078110561, 47)('Dancemaker (1998)', 5.0115251363085775, 45)('\"Decalogue', 4.966019628457392, 498)('Laurence Anyways (2012)', 4.95170892040607, 90)('\"Personal Journey with Martin Scorsese Through American Movies', 4.950990091268236, 33)('\"Century of the Self', 4.943931901519441, 138)(\"Eu Não Quero Voltar Sozinho (I Don't Want to Go Back Alone) (2010)\", 4.936002665483571, 49)('High School (1968)', 4.905992633030817, 30)('Girlhood (2003)', 4.904927636708862, 25)('\"Great War', 4.886188509874742, 20)('Overlord (1975)', 4.875542161536053, 20)('Four Minutes (Vier Minuten) (2006)', 4.872331143616371, 55)('Powers of Ten (1977)', 4.867633081599967, 46)('\"Crucified Lovers', 4.856042214780258, 21)('\"Story of Film: An Odyssey', 4.847967239930643, 52)('Shadows of Our Forgotten Ancestors (Tini zabutykh predkiv) (1964)', 4.845614918754563, 55)('Bill Hicks: Sane Man (1989)', 4.819637752512882, 26)('\"Cremator', 4.818364091923069, 32)(\"Who's Singin' Over There? (a.k.a. Who Sings Over There) (Ko to tamo peva) (1980)\", 4.814951680469164, 30)('Love Streams (1984)', 4.814016517854881, 37)('Hamlet (Gamlet) (1964)', 4.804795065929403, 22)\n"
     ]
    }
   ],
   "source": [
    "new_user_ID = ratings_data.first()[0]\n",
    "print(\"User ID:\", new_user_ID)\n",
    "\n",
    "# Get the list of movies watched by the user\n",
    "new_user_watched_ids = ratings_data.filter(lambda x: x[0] == new_user_ID).map(lambda x: x[0]).collect()\n",
    "\n",
    "# keep just those not on the ID list (thanks Lei Li for spotting the error!)\n",
    "new_user_unrated_movies_RDD = (movies_data.filter(lambda x: x[0] not in new_user_watched_ids).map(lambda x: (new_user_ID, x[0])))\n",
    "\n",
    "# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
    "new_user_recommendations_RDD = complete_model.predictAll(new_user_unrated_movies_RDD)\n",
    "\n",
    "# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n",
    "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_RDD.join(movies_data).join(movie_id_ratings_count)\n",
    "#print(new_user_recommendations_rating_title_and_count_RDD.take(5))\n",
    "\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))\n",
    "    \n",
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=REVIEW_MIN_AMOUNT).takeOrdered(25, key=lambda x: -x[1])\n",
    "\n",
    "print(\"Best recommended movies:\")\n",
    "print(\"\".join(map(str, top_movies)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
